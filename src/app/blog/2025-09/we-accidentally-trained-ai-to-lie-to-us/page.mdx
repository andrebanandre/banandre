---
title: We Accidentally Trained AI to Lie to Us
description: >-
  OpenAI's new confidence-targeted evaluation method reveals we've been
  rewarding LLMs for confident bullshit instead of honest uncertainty
slug: we-accidentally-trained-ai-to-lie-to-us
image: >-
  /blog/2025/addressing-llm-hallucination-new-evaluation-methodologies_llm-hallucinations.webp
date: 2025-09-07T00:00:00.000Z
tags: &ref_0
  - llm
  - hallucination
  - ai-evaluation
  - openai
categories:
  - Artificial Intelligence
author: Banandre
type: article
openGraph:
  type: article
  title: We Accidentally Trained AI to Lie to Us
  description: >-
    OpenAI's new confidence-targeted evaluation method reveals we've been
    rewarding LLMs for confident bullshit instead of honest uncertainty
  url: 'https://banandre.com/blog/2025-09/we-accidentally-trained-ai-to-lie-to-us'
  siteName: Banandre
  images:
    - url: >-
        https://banandre.com/blog/2025/addressing-llm-hallucination-new-evaluation-methodologies_llm-hallucinations.webp
      width: 1200
      height: 630
      alt: We Accidentally Trained AI to Lie to Us
  publishedTime: '2025-09-07T00:00:00.000Z'
  authors:
    - Banandre
  tags: *ref_0
  section: Artificial Intelligence
twitter:
  card: summary_large_image
  site: '@banandre'
  creator: '@banandre'
  title: We Accidentally Trained AI to Lie to Us
  description: >-
    OpenAI's new confidence-targeted evaluation method reveals we've been
    rewarding LLMs for confident bullshit instead of honest uncertainty
  images:
    - >-
      https://banandre.com/blog/2025/addressing-llm-hallucination-new-evaluation-methodologies_llm-hallucinations.webp
jsonLd:
  '@context': 'https://schema.org'
  '@type': BlogPosting
  headline: We Accidentally Trained AI to Lie to Us
  description: >-
    OpenAI's new confidence-targeted evaluation method reveals we've been
    rewarding LLMs for confident bullshit instead of honest uncertainty
  image: >-
    https://banandre.com/blog/2025/addressing-llm-hallucination-new-evaluation-methodologies_llm-hallucinations.webp
  url: 'https://banandre.com/blog/2025-09/we-accidentally-trained-ai-to-lie-to-us'
  datePublished: '2025-09-07T00:00:00.000Z'
  dateModified: '2025-09-07T00:00:00.000Z'
  author:
    '@type': Person
    name: Banandre
    url: 'https://banandre.com'
  publisher:
    '@type': Organization
    name: Banandre
    url: 'https://banandre.com'
    logo:
      '@type': ImageObject
      url: 'https://banandre.com/banana.png'
  mainEntityOfPage:
    '@type': WebPage
    '@id': 'https://banandre.com/blog/2025-09/we-accidentally-trained-ai-to-lie-to-us'
---

The fundamental flaw in today's AI systems isn't that they hallucinate, it's that we've been systematically training them to do so.

## The Confidence Con Game


Large language models don't just randomly invent facts. They've learned to do it because our evaluation systems have been rewarding confident answers over honest ones. When GPT-4 hallucinates at a 28.6% rate or Google's Bard hits a staggering 91.4% error rate in reference generation, we're not seeing system failures, we're seeing exactly what we trained them to do.

The problem stems from how we've structured reinforcement learning. Models get higher scores for providing definitive answers, even when those answers are completely fabricated. Saying "I don't know" typically earns zero points, while making up a plausible-sounding response often gets positive reinforcement. We've essentially created the academic equivalent of the class know-it-all who'd rather bluff than admit ignorance.

## The Behavioral Calibration Breakthrough

[OpenAI's proposed solution](https://share.google/9SKn7X0YThlmnkZ9m) involves what they call "confidence-targeted evaluation", explicitly stating that mistakes should be penalized and admitting uncertainty might receive neutral scores, while guessing incorrectly receives negative points. This encourages "behavioral calibration", where the model only answers when it's sufficiently confident.

The approach recognizes that LLMs don't actually know what they know. They're statistical pattern machines, not reasoning entities. As researchers note, these systems lack true world models and instead rely on statistical approximations of factual relationships. This fundamental limitation results in what's been termed "confident hallucination", where models generate plausible but incorrect information with high apparent certainty.

## Why This Should Have Been Obvious

The irony is that uncertainty modeling isn't new technology. Researchers were doing coupled uncertainty predictions for deep learning models back in 2016. The fact that it's taken until 2025 for major AI labs to implement this basic safety feature speaks volumes about the priorities in AI development.

The current approach has been to prioritize impressive demos over reliability. When your primary metric is "how often does this sound right?" rather than "how often is this actually right?", you're optimizing for the wrong thing. The consequences extend beyond minor inaccuracies, in sectors like healthcare and finance, unreliable AI outputs could endanger lives or finances.

## The Evaluation Revolution


What makes this approach revolutionary isn't just the technical methodology, it's the shift toward continuous evaluation loops. Both NVIDIA and OpenAI are emphasizing data feedback loops (often called "data flywheels") as a way to drive continuous AI improvement. These systems create self-improving cycles where data from AI interactions is continuously used to refine models, leading to better performance over time.

The evaluation-driven approach prevents what engineers call "poke-and-hope guesswork" and replaces impressionistic judgments of accuracy with rigorous measurement. This means we can make principled decisions about cost trade-offs and investment rather than relying on gut feelings about model performance.

---

We've been measuring AI safety completely wrong. We've been obsessed with making models more powerful, more capable, and more human-like in their responses. But we've neglected the most critical feature: knowing when to shut up.

The next generation of AI systems won't be judged by how impressive their answers sound, but by how accurately they can identify their own limitations. The most intelligent response might increasingly be "I don't have enough confidence to answer that", rather than a beautifully crafted fabrication.

We're entering an era where AI humility becomes a feature rather than a bug, where the most advanced systems might actually seem less capable because they've learned to recognize their own boundaries. The real breakthrough isn't making AI smarter, it's making AI smart enough to know its own limitations.
